{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58cef30d-a2b0-4307-8fd3-13e40f1f54b4",
   "metadata": {},
   "source": [
    "# Unstructured playground\n",
    "\n",
    "This notebook can be used to test simple Unstructured features and components in order to get familiarity and understand how they work\n",
    "\n",
    "Unstructured: https://unstructured-io.github.io/unstructured/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04f803c9-ab6c-4149-8289-b97dc576a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "import json\n",
    "\n",
    "from unstructured_client import UnstructuredClient\n",
    "from unstructured_client.models import shared\n",
    "from unstructured_client.models.errors import SDKError\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.partition.pptx import partition_pptx\n",
    "from unstructured.staging.base import dict_to_elements, elements_to_json\n",
    "from unstructured.partition.auto import partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419360ab-f1f8-4186-9dbd-d2cfae6e6132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/davideliu/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "filename = \"documents/medium_blog.html\"\n",
    "elements = partition_html(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d88055b5-3b86-47a4-bb6e-d58e7cbdc585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"29887a5ff9846ccc23327565a07e17fa\",\n",
      "    \"text\": \"Share\",\n",
      "    \"metadata\": {\n",
      "      \"category_depth\": 0,\n",
      "      \"last_modified\": \"2024-04-13T20:07:55\",\n",
      "      \"page_number\": 1,\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"medium_blog.html\",\n",
      "      \"filetype\": \"text/html\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"2bc60d779d5ea8114272b3c498f34643\",\n",
      "    \"text\": \"In the vast digital universe, data is the lifeblood that drives decision-making and innovation. But not all data is created equal. Unstructured data in images and documents often hold a wealth of information that can be challenging to extract and analyze.\",\n",
      "    \"metadata\": {\n",
      "      \"last_modified\": \"2024-04-13T20:07:55\",\n",
      "      \"page_number\": 1,\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"parent_id\": \"29887a5ff9846ccc23327565a07e17fa\",\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"medium_blog.html\",\n",
      "      \"filetype\": \"text/html\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "element_dict = [el.to_dict() for el in elements]\n",
    "example_output = json.dumps(element_dict[11:13], indent=2)\n",
    "print(example_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b4da6dc-b1f0-4b56-a901-00536014aa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"50a4122943273ad2f00ea92bff9c7cb6\",\n",
      "    \"text\": \"ChatGPT\",\n",
      "    \"metadata\": {\n",
      "      \"category_depth\": 1,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"msft_openai.pptx\",\n",
      "      \"last_modified\": \"2024-04-13T20:09:24\",\n",
      "      \"page_number\": 1,\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"ListItem\",\n",
      "    \"element_id\": \"62ec8febb7453876bd797cd6cd38ada4\",\n",
      "    \"text\": \"Chat-GPT: AI Chatbot, developed by OpenAI,\\u00a0trained to perform conversational tasks and\\u00a0creative tasks\",\n",
      "    \"metadata\": {\n",
      "      \"category_depth\": 0,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"msft_openai.pptx\",\n",
      "      \"last_modified\": \"2024-04-13T20:09:24\",\n",
      "      \"page_number\": 1,\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"parent_id\": \"50a4122943273ad2f00ea92bff9c7cb6\",\n",
      "      \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"ListItem\",\n",
      "    \"element_id\": \"a65487e7a89151a78c3ee11901bfe432\",\n",
      "    \"text\": \"Backed by GPT-3.5 model (gpt-35-turbo), GPT-4\\u00a0models\",\n",
      "    \"metadata\": {\n",
      "      \"category_depth\": 0,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"msft_openai.pptx\",\n",
      "      \"last_modified\": \"2024-04-13T20:09:24\",\n",
      "      \"page_number\": 1,\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"parent_id\": \"50a4122943273ad2f00ea92bff9c7cb6\",\n",
      "      \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "filename = \"documents/msft_openai.pptx\"\n",
    "elements = partition_pptx(filename=filename)\n",
    "element_dict = [el.to_dict() for el in elements]\n",
    "output = json.dumps(element_dict[:3], indent=2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66e0cb6a-a31e-47e5-a2be-7f49e7f78051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"bff1fd0ec25e78f1224ad7309a1e79c4\",\n",
      "    \"text\": \"B All Experimental Results\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.9187041521072388,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            298.23516845703125,\n",
      "            205.790342222222\n",
      "          ],\n",
      "          [\n",
      "            298.23516845703125,\n",
      "            238.99923111111084\n",
      "          ],\n",
      "          [\n",
      "            713.6279296875,\n",
      "            238.99923111111084\n",
      "          ],\n",
      "          [\n",
      "            713.6279296875,\n",
      "            205.790342222222\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1700,\n",
      "        \"layout_height\": 2200\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-13T20:07:53\",\n",
      "      \"filetype\": \"application/pdf\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"CoT.pdf\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"ebf8dfb149bcbbd8c4b7f9a7046900a9\",\n",
      "    \"text\": \"This section contains tables for experimental results for varying models and model sizes, on all benchmarks, for standard prompting vs. chain-of-thought prompting.\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.9404913187026978,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            295.14404296875,\n",
      "            271.98025166666656\n",
      "          ],\n",
      "          [\n",
      "            295.14404296875,\n",
      "            329.9569183333334\n",
      "          ],\n",
      "          [\n",
      "            1409.8409423828125,\n",
      "            329.9569183333334\n",
      "          ],\n",
      "          [\n",
      "            1409.8409423828125,\n",
      "            271.98025166666656\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1700,\n",
      "        \"layout_height\": 2200\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-13T20:07:53\",\n",
      "      \"filetype\": \"application/pdf\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"bff1fd0ec25e78f1224ad7309a1e79c4\",\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"CoT.pdf\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"6cd0bd087c9bbab6e4d5d9b82c81a73d\",\n",
      "    \"text\": \"For the arithmetic reasoning benchmarks, some chains of thought (along with the equations produced) were correct, except the model performed an arithmetic operation incorrectly. A similar observation was made in Cobbe et al. (2021). Hence, we can further add a Python program as an external calculator (using the Python eval function) to all the equations in the generated chain of thought. When there are multiple equations in a chain of thought, we propagate the external calculator results from one equation to the following equations via string matching. As shown in Table 1, we see that adding a calculator signi\\ufb01cantly boosts performance of chain-of-thought prompting on most tasks.\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.9474514126777649,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            298.7,\n",
      "            347.80802944444434\n",
      "          ],\n",
      "          [\n",
      "            298.7,\n",
      "            557.2985849999999\n",
      "          ],\n",
      "          [\n",
      "            1404.8353126388886,\n",
      "            557.2985849999999\n",
      "          ],\n",
      "          [\n",
      "            1404.8353126388886,\n",
      "            347.80802944444434\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1700,\n",
      "        \"layout_height\": 2200\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-13T20:07:53\",\n",
      "      \"filetype\": \"application/pdf\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"bff1fd0ec25e78f1224ad7309a1e79c4\",\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"CoT.pdf\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"dcbba23d31f0f544be8cd44ada08a0e3\",\n",
      "    \"text\": \"Table 1: Chain of thought prompting outperforms standard prompting for various large language models on \\ufb01ve arithmetic reasoning benchmarks. All metrics are accuracy (%). Ext. calc.: post-hoc external calculator for arithmetic computations only. Prior best numbers are from the following. a: Cobbe et al. (2021). b & e: Pi et al. (2022), c: Lan et al. (2021), d: Pi\\u02dbekos et al. (2021).\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.9191029667854309,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            297.3262939453125,\n",
      "            606.960807222222\n",
      "          ],\n",
      "          [\n",
      "            297.3262939453125,\n",
      "            725.5430294444443\n",
      "          ],\n",
      "          [\n",
      "            1407.729248046875,\n",
      "            725.5430294444443\n",
      "          ],\n",
      "          [\n",
      "            1407.729248046875,\n",
      "            606.960807222222\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1700,\n",
      "        \"layout_height\": 2200\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-13T20:07:53\",\n",
      "      \"filetype\": \"application/pdf\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"bff1fd0ec25e78f1224ad7309a1e79c4\",\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"CoT.pdf\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Table\",\n",
      "    \"element_id\": \"6cf6062280937865f52050dc49158ec4\",\n",
      "    \"text\": \"Prior best Prompting N/A (\\ufb01netuning) 55a GSM8K SVAMP ASDiv 57.4b 75.3c AQuA 37.9d MAWPS 88.4e UL2 20B Standard Chain of thought 4.4 (+0.3) + ext. calc 4.1 6.9 10.1 12.5 (+2.4) 16.9 (+0.9) 23.6 (+3.1) 28.3 16.0 20.5 34.3 23.6 16.6 19.1 (+2.5) 42.7 LaMDA 137B Standard Chain of thought 14.3 (+7.8) + ext. calc 6.5 17.8 29.5 37.5 (+8.0) 46.6 (+6.5) 20.6 (-4.9) 42.1 40.1 25.5 53.4 20.6 43.2 57.9 (+14.7) 69.3 GPT-3 175B (text-davinci-002) Chain of thought 46.9 (+31.3) 68.9 (+3.2) 71.3 (+1.0) 35.8 (+11.0) 87.1 (+14.4) Standard 15.6 65.7 70.3 24.8 72.7 + ext. calc 49.6 70.3 71.1 35.8 87.5 Codex (code-davinci-002) Chain of thought 63.1 (+43.4) 76.4 (+6.5) 80.4 (+6.4) 45.3 (+15.8) 92.6 (+13.9) Standard 19.7 69.9 74.0 29.5 78.7 + ext. calc 65.4 77.0 80.0 45.3 93.3 PaLM 540B Standard Chain of thought 56.9 (+39.0) 79.0 (+9.6) 73.9 (+1.8) 35.8 (+10.6) 93.3 (+14.2) + ext. calc 17.9 69.4 72.1 25.2 79.2 58.6 79.8 72.6 35.8 93.5\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.9267750382423401,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            320.5267333984375,\n",
      "            731.715087890625\n",
      "          ],\n",
      "          [\n",
      "            320.5267333984375,\n",
      "            1387.8294677734375\n",
      "          ],\n",
      "          [\n",
      "            1368.7069091796875,\n",
      "            1387.8294677734375\n",
      "          ],\n",
      "          [\n",
      "            1368.7069091796875,\n",
      "            731.715087890625\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1700,\n",
      "        \"layout_height\": 2200\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-13T20:07:53\",\n",
      "      \"text_as_html\": \"<table><tr><td>Prior best</td><td>Prompting (finetuning)</td><td>GSM8K 55%</td><td>SVAMP 57.4\\u00b0</td><td>ASDiv 75.3\\u00b0</td><td>AQuA 37.9\\u00a2</td><td>MAWPS 88.4\\u00b0</td></tr><tr><td>UL2 20B</td><td>Standard Chain of thought</td><td>4.1 4.4 (+03)</td><td>10.1 12.5 (42.4)</td><td>16.0 16.9 (40.9)</td><td>20.5 23.6 43.1)</td><td>16.6 19.1 (42.5)</td></tr><tr><td></td><td>+ ext, calc</td><td>6.9</td><td>28.3</td><td>34.3</td><td>23.6</td><td>42.7</td></tr><tr><td>LaMDA 137B</td><td>Standard Chain of thought</td><td>6.5 14.3 47.8)</td><td>29.5 37.5 48.0)</td><td>40.1 46.6 (46.5)</td><td>25.5 20.6 (-4.9)</td><td>43.2 57.9 (414.7)</td></tr><tr><td></td><td>+ ext, calc</td><td>178</td><td>42.1</td><td>53.4</td><td>20.6</td><td>69.3</td></tr><tr><td>GPT-3 175B (text-davinci-002)</td><td>Standard Chain of thought</td><td>15.6 46.9 (+313)</td><td>65.7 68.9 (43.2)</td><td>70.3 71.3 41.0)</td><td>24.8 35.8 (11.0)</td><td>72.7 87.1 @14.4)</td></tr><tr><td></td><td>+ ext, calc</td><td>49.6</td><td>70.3</td><td>71d</td><td>35.8</td><td>87.5</td></tr><tr><td>Codex</td><td>Standard</td><td>19.7</td><td>69.9</td><td>74.0</td><td>29.5</td><td>78.7</td></tr><tr><td>(code-davinci-002)</td><td>Chain of thought</td><td>63.1 (+43.4)</td><td>76.4 (46.5)</td><td>80.4 (46.4)</td><td>45.3 (415.8)</td><td>92.6 (413.9)</td></tr><tr><td></td><td>+ ext, calc</td><td>65.4</td><td>71.0</td><td>80.0</td><td>45.3</td><td>93.3</td></tr><tr><td>PaLM 540B</td><td>Standard</td><td>17.9</td><td>69.4</td><td>72.1</td><td>25.2</td><td>79.2</td></tr><tr><td></td><td>Chain of thought</td><td>56.9 (+39.0)</td><td>79.0 (49.6)</td><td>73.9 (41.8)</td><td>35.8 (+106)</td><td>93.3 (414.2)</td></tr><tr><td></td><td>+ ext, calc</td><td>58.6</td><td>798</td><td>72.6</td><td>35.8</td><td>93.5</td></tr></table>\",\n",
      "      \"filetype\": \"application/pdf\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"bff1fd0ec25e78f1224ad7309a1e79c4\",\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"CoT.pdf\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Header\",\n",
      "    \"element_id\": \"c7fee3dd719180a66847709f73429630\",\n",
      "    \"text\": \"20\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.609601616859436,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            835.84375,\n",
      "            2063.124696111111\n",
      "          ],\n",
      "          [\n",
      "            835.84375,\n",
      "            2090.798585\n",
      "          ],\n",
      "          [\n",
      "            865.3739624023438,\n",
      "            2090.798585\n",
      "          ],\n",
      "          [\n",
      "            865.3739624023438,\n",
      "            2063.124696111111\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1700,\n",
      "        \"layout_height\": 2200\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-13T20:07:53\",\n",
      "      \"filetype\": \"application/pdf\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"CoT.pdf\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "filename = \"documents/CoT.pdf\"\n",
    "elements = partition(filename)\n",
    "element_dict = [el.to_dict() for el in elements]\n",
    "output = json.dumps(element_dict[:], indent=2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b378ba4-670e-4ce1-8537-d265cd62db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B All Experimental Results\n",
      "\n",
      "This section contains tables for experimental results for varying models and model sizes, on all benchmarks, for standard prompting vs. chain-of-thought prompting.\n",
      "\n",
      "For the arithmetic reasoning benchmarks, some chains of thought (along with the equations produced) were correct, except the model performed an arithmetic operation incorrectly. A similar observation was made in Cobbe et al. (2021). Hence, we can further add a Python program as an external calculator (using the Python eval function) to all the equations in the generated chain of thought. When there are multiple equations in a chain of thought, we propagate the external calculator results from one equation to the following equations via string matching. As shown in Table 1, we see that adding a calculator signiﬁcantly boosts performance of chain-of-thought prompting on most tasks.\n",
      "\n",
      "Table 1: Chain of thought prompting outperforms standard prompting for various large language models on ﬁve arithmetic reasoning benchmarks. All metrics are accuracy (%). Ext. calc.: post-hoc external calculator for arithmetic computations only. Prior best numbers are from the following. a: Cobbe et al. (2021). b & e: Pi et al. (2022), c: Lan et al. (2021), d: Pi˛ekos et al. (2021).\n",
      "\n",
      "Prior best Prompting N/A (ﬁnetuning) 55a GSM8K SVAMP ASDiv 57.4b 75.3c AQuA 37.9d MAWPS 88.4e UL2 20B Standard Chain of thought 4.4 (+0.3) + ext. calc 4.1 6.9 10.1 12.5 (+2.4) 16.9 (+0.9) 23.6 (+3.1) 28.3 16.0 20.5 34.3 23.6 16.6 19.1 (+2.5) 42.7 LaMDA 137B Standard Chain of thought 14.3 (+7.8) + ext. calc 6.5 17.8 29.5 37.5 (+8.0) 46.6 (+6.5) 20.6 (-4.9) 42.1 40.1 25.5 53.4 20.6 43.2 57.9 (+14.7) 69.3 GPT-3 175B (text-davinci-002) Chain of thought 46.9 (+31.3) 68.9 (+3.2) 71.3 (+1.0) 35.8 (+11.0) 87.1 (+14.4) Standard 15.6 65.7 70.3 24.8 72.7 + ext. calc 49.6 70.3 71.1 35.8 87.5 Codex (code-davinci-002) Chain of thought 63.1 (+43.4) 76.4 (+6.5) 80.4 (+6.4) 45.3 (+15.8) 92.6 (+13.9) Standard 19.7 69.9 74.0 29.5 78.7 + ext. calc 65.4 77.0 80.0 45.3 93.3 PaLM 540B Standard Chain of thought 56.9 (+39.0) 79.0 (+9.6) 73.9 (+1.8) 35.8 (+10.6) 93.3 (+14.2) + ext. calc 17.9 69.4 72.1 25.2 79.2 58.6 79.8 72.6 35.8 93.5\n",
      "\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join([str(el) for el in elements]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1af7f45d-5b8a-425e-b29d-23e37f615428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"type\": \"Header\",\n",
      "    \"element_id\": \"d2592b60ee5cdc4d0f8c790be927c679\",\n",
      "    \"text\": \"Nx\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.4063887298107147,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            42.20499801635742,\n",
      "            1228.7130126953125\n",
      "          ],\n",
      "          [\n",
      "            42.20499801635742,\n",
      "            1286.28515625\n",
      "          ],\n",
      "          [\n",
      "            124.905517578125,\n",
      "            1286.28515625\n",
      "          ],\n",
      "          [\n",
      "            124.905517578125,\n",
      "            1228.7130126953125\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1520,\n",
      "        \"layout_height\": 2239\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-14T15:25:21\",\n",
      "      \"filetype\": \"image/png\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"transformer.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Image\",\n",
      "    \"element_id\": \"d333b59532c27796553fef06ae6cf16c\",\n",
      "    \"text\": \"Add & Norm | Gada. Norm Add & Norm Multi- Head Attention SE a, @ \\u00a9\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.3419244885444641,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            173.2862548828125,\n",
      "            802.7903442382812\n",
      "          ],\n",
      "          [\n",
      "            173.2862548828125,\n",
      "            1725.53125\n",
      "          ],\n",
      "          [\n",
      "            709.252197265625,\n",
      "            1725.53125\n",
      "          ],\n",
      "          [\n",
      "            709.252197265625,\n",
      "            802.7903442382812\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1520,\n",
      "        \"layout_height\": 2239\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-14T15:25:21\",\n",
      "      \"filetype\": \"image/png\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"transformer.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"428ace734291df8c05e94af3d8d5c215\",\n",
      "    \"text\": \"Positional Encoding\",\n",
      "    \"metadata\": {\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            4.0,\n",
      "            1657.0\n",
      "          ],\n",
      "          [\n",
      "            4.0,\n",
      "            1798.0\n",
      "          ],\n",
      "          [\n",
      "            252.0,\n",
      "            1798.0\n",
      "          ],\n",
      "          [\n",
      "            252.0,\n",
      "            1657.0\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1520,\n",
      "        \"layout_height\": 2239\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-14T15:25:21\",\n",
      "      \"filetype\": \"image/png\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"transformer.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"5eebcb192a20ace2847e7d78120b4026\",\n",
      "    \"text\": \"Input Embedding\",\n",
      "    \"metadata\": {\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            362.0,\n",
      "            1835.0\n",
      "          ],\n",
      "          [\n",
      "            362.0,\n",
      "            1953.0\n",
      "          ],\n",
      "          [\n",
      "            617.0,\n",
      "            1953.0\n",
      "          ],\n",
      "          [\n",
      "            617.0,\n",
      "            1835.0\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1520,\n",
      "        \"layout_height\": 2239\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-14T15:25:21\",\n",
      "      \"filetype\": \"image/png\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"transformer.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"7abc49dfa87bc183179762b87f9914ce\",\n",
      "    \"text\": \"Inputs\",\n",
      "    \"metadata\": {\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            411.0,\n",
      "            2102.0\n",
      "          ],\n",
      "          [\n",
      "            411.0,\n",
      "            2158.0\n",
      "          ],\n",
      "          [\n",
      "            569.0,\n",
      "            2158.0\n",
      "          ],\n",
      "          [\n",
      "            569.0,\n",
      "            2102.0\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1520,\n",
      "        \"layout_height\": 2239\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-14T15:25:21\",\n",
      "      \"filetype\": \"image/png\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"transformer.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"b2439bcb8dee14b685f137f294b0e0cb\",\n",
      "    \"text\": \"Output\",\n",
      "    \"metadata\": {\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            939.0,\n",
      "            8.0\n",
      "          ],\n",
      "          [\n",
      "            939.0,\n",
      "            65.0\n",
      "          ],\n",
      "          [\n",
      "            1120.0,\n",
      "            65.0\n",
      "          ],\n",
      "          [\n",
      "            1120.0,\n",
      "            8.0\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1520,\n",
      "        \"layout_height\": 2239\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-14T15:25:21\",\n",
      "      \"filetype\": \"image/png\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"transformer.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Image\",\n",
      "    \"element_id\": \"c402d503276a622926f4684fca8ad546\",\n",
      "    \"text\": \"Probabilities Add & Norm Feed Forward Add & Norm Multi- Head Attention Add & Norm Masked Multi-Head Attention of\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.3303276598453522,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            752.310302734375,\n",
      "            90.00225067138672\n",
      "          ],\n",
      "          [\n",
      "            752.310302734375,\n",
      "            1635.139404296875\n",
      "          ],\n",
      "          [\n",
      "            1376.6492919921875,\n",
      "            1635.139404296875\n",
      "          ],\n",
      "          [\n",
      "            1376.6492919921875,\n",
      "            90.00225067138672\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1520,\n",
      "        \"layout_height\": 2239\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-14T15:25:21\",\n",
      "      \"filetype\": \"image/png\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"transformer.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"9c7c8fef33a247fd1e0ffd224c1b78c3\",\n",
      "    \"text\": \"Positional @ Encoding Output Embedding\",\n",
      "    \"metadata\": {\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            902.0,\n",
      "            1672.0\n",
      "          ],\n",
      "          [\n",
      "            902.0,\n",
      "            1948.0\n",
      "          ],\n",
      "          [\n",
      "            1516.0,\n",
      "            1948.0\n",
      "          ],\n",
      "          [\n",
      "            1516.0,\n",
      "            1672.0\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1520,\n",
      "        \"layout_height\": 2239\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-14T15:25:21\",\n",
      "      \"filetype\": \"image/png\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"transformer.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"UncategorizedText\",\n",
      "    \"element_id\": \"0a79b0ca699d37228d3affd3b6d38b7f\",\n",
      "    \"text\": \"\\u00a9\",\n",
      "    \"metadata\": {\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            1005.0,\n",
      "            1710.0\n",
      "          ],\n",
      "          [\n",
      "            1005.0,\n",
      "            1759.0\n",
      "          ],\n",
      "          [\n",
      "            1051.0,\n",
      "            1759.0\n",
      "          ],\n",
      "          [\n",
      "            1051.0,\n",
      "            1710.0\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1520,\n",
      "        \"layout_height\": 2239\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-14T15:25:21\",\n",
      "      \"filetype\": \"image/png\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"9c7c8fef33a247fd1e0ffd224c1b78c3\",\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"transformer.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"8d871ae6dcfd442b998ea2167eb87767\",\n",
      "    \"text\": \"Outputs (shifted right)\",\n",
      "    \"metadata\": {\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            862.0,\n",
      "            2101.0\n",
      "          ],\n",
      "          [\n",
      "            862.0,\n",
      "            2233.0\n",
      "          ],\n",
      "          [\n",
      "            1196.0,\n",
      "            2233.0\n",
      "          ],\n",
      "          [\n",
      "            1196.0,\n",
      "            2101.0\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1520,\n",
      "        \"layout_height\": 2239\n",
      "      },\n",
      "      \"last_modified\": \"2024-04-14T15:25:21\",\n",
      "      \"filetype\": \"image/png\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"9c7c8fef33a247fd1e0ffd224c1b78c3\",\n",
      "      \"file_directory\": \"documents\",\n",
      "      \"filename\": \"transformer.png\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "filename = \"documents/transformer.png\"\n",
    "elements = partition(filename)\n",
    "element_dict = [el.to_dict() for el in elements]\n",
    "output = json.dumps(element_dict[:], indent=2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271dbfe-febb-4869-905a-2a92c7d552ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_testing",
   "language": "python",
   "name": "llm_testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
